---
layout: default
root: ""
---

<!-- Main component for a primary marketing message or call to action -->
<div>
  <h1>Some of Entity Resolution</h1>
</div>
<div class="jumbotron">
  <blockquote>
    <p>Data! Data! Data! I canâ€™t make bricks without clay!</p>
    <small>Sherlock Holmes<cite title="The Adventure of the Copper Beeches"> The Adventure of the Copper Beeches</cite></small>
  </blockquote>
</div>
<div>
  <h4>Workshop objectives</h4>
  <p>Very often information about social entities is scattered across multiple databases. Combining that information into one database can result in enormous benefits for analysis, resulting in richer and more reliable conclusions. Among the types of questions that have been, and can be, addressed by combining information include: How accurate are census enumerations for minority groups? How many of the elderly are at high risk for sepsis in different parts of the country? How many people were victims of war crimes in recent conflicts in Syria? In most practical applications, however, analysts cannot simply link records across databases based on unique identifiers, such as social security numbers, either because they are not a part of some databases or are not available due to privacy concerns. In such cases, analysts need to use methods from statistical and computational science known as entity resolution (record linkage or de-duplication) to proceed with analysis. Entity resolution is not only a crucial task for social science and industrial applications, but is a challenging statistical and computational problem itself. In this short course, we first provide an overview and introduction to entity resolution. Second, we provide an introduction to computational speed-ups, known as blocking or partitioning. Third, we introduce a sub-quadratic type of blocking - locality sensitive hashing - which allows one to place similar entities into blocks. Fourth, we illustrate how locality sensitive hashing can be used for unique estimating the number of documented identifiable deaths in a subset of the Syrian conflict using new methodology. Fifth, we provide an introduction to Bayesian entity resolution, which allows one to propagate the entity resolution error exactly into an subsequent process. In the workshop, demos will be given using open source software.
  </p>
</div>
<br/>
<div>
  <h2>Workshop Topics:</h2>
  <table class="table table-striped table-hover">
    <thead>
      <tr>
        <th>Topic</th>
        <th>Description</th>
        <th>Materials</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>I. Overview and introduction to record linkage</td>
        <td><p>Very often information about social entities is scattered across multiple databases. Combining that information into one database can result in enormous benefits for analysis, resulting in richer and more reliable conclusions. Among the types of questions that have been, and can be, addressed by combining information include: How accurate are census enumerations for minority groups? How many of the elderly are at high risk for sepsis in different parts of the country ? How many people were victims of war crimes in recent conflicts in Syria? In most practical applications, however, analysts cannot simply link records across databases based on unique identifiers, such as social security numbers, either because they are not a part of some databases or are not available due to privacy concerns. In such cases, analysts need to use methods from statistical and computational science known as record linkage (also called entity resolution or de-duplication) to proceed with analysis. Record linkage is not only a crucial task for social science and industrial applications, but is a challenging statistical and computational problem itself, because many databases contain errors (noise, lies, omissions, duplications, etc.), and the number of parameters to be estimated grows with the number of records. To meet present and near-future needs, record linkage methods must be flexible and scalable to large databases; furthermore, they must be able to handle uncertainty and be easily integrated with post-linkage statistical analyses, such as logistic regression or capture recapture. All this must be done while maintaining accuracy and low error rates.</p></td>
        <td>
          <a href="">slides</a> | <a href="">code</a>
        </td>
      </tr>
    </tbody>
  </table>
</div>
