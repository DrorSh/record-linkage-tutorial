---
title: "Introduction to Bayesian record linkage "
author: Brenda Betancourt and Andee Kaplan
institute: |
    | Duke University
    | Department of Statistical Science
shortinstitute: Duke University
date: |
  | February 8, 2018
  |
  | Slides available at <http://bit.ly/cimat-bayes>
  |
output: 
  beamer_presentation:
    keep_tex: false
    template: beamer.tex
fig_caption: true
classoption: compress
natbib: true
---

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(ggplot2)

opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
theme_set(theme_bw(base_family = "serif", base_size = 30))
```

# What is "Bayesian"?

1. Setting up a *full probability model* -- a joint probability distribution for all observable and unobservable quantities
    \begin{align*}
    p(\boldsymbol x | \boldsymbol \theta) &- \text{ likelihood} \\
    p(\boldsymbol \theta) &- \text{ prior}
    \end{align*}
2. Conditioning on observed data -- calculating and interpreting the appropriate *posterior distribution*
    $$
    p(\boldsymbol \theta | \boldsymbol x) = \frac{p(\boldsymbol x, \boldsymbol \theta)}{p(\boldsymbol x)} = \frac{p(\boldsymbol x|\boldsymbol \theta)p(\boldsymbol \theta)}{p(\boldsymbol x)} \propto p(\boldsymbol x|\boldsymbol \theta)p(\boldsymbol \theta)
    $$

# Why Bayes?

# Common approaches

Felligi-Sutner  
Clustering  
Beka's  

# Empirical Bayes graphical model for record linkage


# `blink` package

`R` package that removes duplicate entries from multiple databses using the empirical Bayes graphical method:

```{r blink-install, eval=FALSE, echo=TRUE}
install.packages("blink")
```

- Formatting data for use with `blink`
- Tuning parameters
- Running the Gibbs sampler (estimate model parameters)
- Output


# `RLdata500` data

We will continue with the  `RLdata500` dataset in the `RecordLinkage` package consisting of $500$ records with 10% duplication.

```{r data, echo=TRUE}
library(blink) # load blink library
library(RecordLinkage) # load data library
data("RLdata500") # load data
head(RLdata500) # take a look
```

# Formatting the data

```{r, echo=TRUE}
# categorical variables
X.c <- as.matrix(RLdata500[, c("by","bm","bd")]) 
p.c <- ncol(X.c) 

# string variables 
X.s <- as.matrix(RLdata500[, c("fname_c1", "lname_c1")]) 
p.s <- ncol(X.s) 
```

`X.c` and `X.s` include all files stacked on top of each other, for categorical and string variables respectively

```{r echo=TRUE}
# keep track of which rows of are in which files
file.num <- rep(c(1, 2, 3), c(200, 150, 150))
```

# Tuning parameters

## Hyperparameters

```{r, echo=TRUE}
# Subjective choices for distortion probability prior
# parameters of a Beta(a,b)
a <- 1
b <- 999
```

## Distortion

```{r, echo=TRUE}
# string distance function example
d <- function(s1, s2) {
  adist(s1, s2) # approximate string distance
}

# steepness parameter
c <- 1
```

# Running the Gibbs sampler

```{r, echo=TRUE, cache=TRUE, results='hide'}
lam.gs <- rl.gibbs(file.num = file.num, # file
                   X.s = X.s, X.c = X.c, # data 
                   num.gs = 100000, # iterations
                   a = a, b = b, # prior params
                   c = c, d = d, # distortion
                   M = 500) # max # latents
```

# Output

\scriptsize

```{r get-estimates-sizes, echo=TRUE, results="hide"}
# count how many unique latent individuals
size_est <- apply(lam.gs, 1, function(x) { 
  length(unique(x)) 
  })
```

```{r plots, fig.width=2.5, fig.height=2.5, fig.show='hold'}
theme_set(theme_bw(base_family = "serif", base_size = 15))

ggplot() +
  geom_line(aes(1:100000, size_est)) +
  xlab("Iteration") +
  ylab("Estimated size")

ggplot() +
  geom_histogram(aes(size_est[-(1:50000)])) +
  geom_vline(aes(xintercept=450), colour = "red") +
  geom_vline(aes(xintercept=mean(size_est)), lty=2) +
  xlab("Estimated size")
  
```

# Evaluation
\scriptsize
```{r, echo=TRUE}
# estimated pairwise links
est_links_pair <- pairwise(links(lam.gs[-(1:50000), ]))

# true pairwise links
true_links_pair <- pairwise(links(matrix(identity.RLdata500, nrow = 1)))

#comparison
comparison <- links.compare(est_links_pair, true_links_pair, counts.only = TRUE)

# false positive rate
fpr <- comparison$incorrect/comparison$correct

# false negative rate
fnr <- comparison$missing/comparison$correct

# false discovery rate
fdr <- comparison$incorrect/(comparison$correct + comparison$incorrect)

# results
c(fpr, fnr, fdr)
```


# Your turn



