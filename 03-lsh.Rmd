---
title: "Introduction to locality sensitive hashing"
author: Andee Kaplan
institute: |
    | Duke University
    | Department of Statistical Science
    | andrea.kaplan@duke.edu
shortinstitute: andrea.kaplan@duke.edu
date: |
  | February 8, 2018
  |
  | Slides available at <http://bit.ly/cimat-lsh>
  |
output: 
  beamer_presentation:
    keep_tex: false
    template: beamer.tex
fig_caption: true
classoption: compress
natbib: true
---

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(ggplot2)
library(RecordLinkage)

opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
```

# Goal and outline

**Goal: ** Introduce locality sensitive hashing, a fast method of blocking for record linkage, and get some experience doing LSH in `R`.

\vfill
1. Defining similarity
\vfill
1. Representing data as sets (shingling)
\vfill
1. Hashing
\vfill
1. Hashing with compression (minhashing)
\vfill
1. Too many pairs to compare! (LSH)
\vfill
1. Evaluation
\vfill

# Finding similar items

- We want to find similar items
\vfill
    - Maybe we are looking for near duplicate documents (plagiarism)
    \vfill
    - More likely, we are trying to block our data which we can later pass to a record linkage process
    \vfill
- How do we define *similar*?
\vfill

# Jaccard similarity

There are many ways to define similarity, we will use *Jaccard similarity* for this task.
$$
Jac(S, T) = \frac{\mid S \cap T\mid}{\mid S \cup T \mid}
$$

 
\begin{figure}[h]
\centering
\includegraphics[width=.5\textwidth]{images/jaccard}
\caption{Two sets S and T with Jaccard similarity 3/7. The two sets share 3 elements in common, and there are 7 elements in total.}
\label{fig:jaccard}
\end{figure}

# How to represent data as sets

We want to talk about similarity of data $\Rightarrow$ we need sets to compare!

- One way is to construct from the data the set of **short strings** that appear within it
\vfill
- Similar documents/datasets will have many common elements, i.e. many commong short strings
\vfill
- We can do construct these short strings using *shingling*
\vfill

# $k$-shingling (how-to)

1. Think of a document or record as a string of characters
\vfill
2. A $k$-shingle (k-gram) is any sub-string (word) of length $k$ found within the document or record
\vfill
3. Associate with each document or record the set of $k$-shingles that appear one or more times within it
\vfill

# Let's try

Suppose our document is the string "Hello world" and $k  = 2$, then 

- the set of $2$-shingles is $\{\text{he, el, ll, lo, ow, wo, or, rl, ld}\}$
\vfill
- the set of $3$-shingles is $\{\text{hel, ell, llo, low, owo, wor, orl, rld}\}$
\vfill

# Your turn

We have the following two records:

```{r your-turn1}
data("RLdata500")
records <- RLdata500[129:130, c(1,3)]
names(records) <- c("First name", "Last name")
kable(records)
```

1. Compute the $2$-shingles for each record
\vfill
2. Using Jaccard similarity, how similar are they?
\vfill

# Your turn solution
\vfill
1. The $2$-shingles for the first record are $\{\text{mi, ic, ch, ha, ae, el, lv, vo, og, ge, el}\}$ and for the second are $\{\text{mi, ic, ch, ha, ae, el, lm, me, ey, ye, er}\}$.
\vfill
2. There are 6 items in common $\{\text{mi, ic, ch, ha, ae, el}\}$ and 16 items total $\{\text{mi, ic, ch, ha, ae, el, lv, vo, og, ge, el, lm, me, ey, ye, er}\}$, so the Jaccard similarity is $\frac{6}{16} = \frac{3}{8} = `r round(3/8, 4)`$
\vfill

# Useful packages/functions in `R`

(Obviously) We don't want to do this by hand most times. Here are some useful packages in `R` that can help us!

```{r helpful-packages, echo = TRUE}
# detecting text reuse and document similarity + shingles
library(textreuse) 
library(tokenizers)
```

We can use the following functions to create $k$-shingles and calculate Jaccard similarity for our data

```{r helpful-functions, eval=FALSE, echo=TRUE}
# get k-shingles
tokenize_character_shingles(x, n)

# calculate jaccard similarity for two sets
jaccard_similarity(a, b) 
```

# Example data

Research paper headers and citations, with information on authors, title, institutions, venue, date, page numbers and several other fields.

\tiny
```{r load-ex-data, echo=TRUE}
library(RLdata)
data(cora)
str(cora)
```

# Your turn 

Using the `title`, `authors`, and `journal` fields in the `cora` dataset,

\vfill
1. Get the $3$-shingles for each record (**hint:** use `tokenize_character_shingles`).
\vfill
2. Obtain the Jaccard similarity between each pair of records (**hint:** use `jaccard_similarity`).
\vfill

# Your turn solution

\tiny

```{r your-turn2-sol, echo=TRUE, cache=TRUE}
# get only the columns we want
dat <- cora[, c("title", "authors", "journal")]

# 1. paste the columns together and tokenize for each record
shingles <- apply(dat, 1, function(x) {
  tokenize_character_shingles(paste(x, collapse=" "), n = 3)[[1]]
})

# 2. Jaccard similarity between pairs
jaccard <- expand.grid(record1 = seq_len(nrow(dat)),
                       record2 = seq_len(nrow(dat)))

# don't need to compare the same things twice
jaccard <- jaccard[jaccard$record1 < jaccard$record2,]

time <- Sys.time()
jaccard$similarity <- apply(jaccard, 1, function(pair) {
  jaccard_similarity(shingles[[pair[1]]], shingles[[pair[2]]])
})
time <- difftime(Sys.time(), time, units = "secs")
```

\normalsize
This took took $`r round(time, 2)`$ seconds $\approx `r round(time/(60), 2)`$ minutes

# Your turn solution (cont'd)

```{r your-turn2-plot}
ggplot(jaccard) +
  geom_raster(aes(x = record1, y = record2, fill=similarity)) +
  theme(aspect.ratio = 1)
```

# Hashing

For a dataset of size $n$, the number of comparisons we must compute is $\frac{n(n-1)}{2}$. 

\vfill
- For our set of records, we needed to compute $`r scales::comma(nrow(dat)*(nrow(dat) - 1)/2)`$ comparisons
\vfill
- A better approach for datasets of any realistic size is to use *hashing*
\vfill

# Hash functions

- Traditionally, a *hash function* maps objects to integers such that similar objects are far apart
- Instead, we want special hash functions that do the **opposite** of this, i.e. similar objects are placed closed together!


## Definition: Hash function
*Hash functions* $h()$ are defined such that

> If records $A$ and $B$ have high similarity, then the probability that $h(A) = h(B)$ is **high** and if records $A$ and $B$ have low similarity, then the probability that $h(A) \not= h(B)$ is **high**.

# Hashing shingles

Instead of storing the strings (shingles), we can just store the *hashed values*  

These are integers, they will take less space

\footnotesize

```{r hash-tokens, echo=TRUE}
# instead store hash values (less memory)
hashed_shingles <- apply(dat, 1, function(x) {
  string <- paste(x, collapse=" ")
  shingles <- tokenize_character_shingles(string, n = 3)[[1]]
  hash_string(shingles)
})
```

```{r hash-tokens-jaccard, cache=TRUE}
# Jaccard similarity on hashed shingles
hashed_jaccard <- expand.grid(record1 = seq_len(nrow(dat)),
                       record2 = seq_len(nrow(dat)))

# don't need to compare the same things twice
hashed_jaccard <- hashed_jaccard[hashed_jaccard$record1 < hashed_jaccard$record2,]

time <- Sys.time()
hashed_jaccard$similarity <- apply(hashed_jaccard, 1, function(pair) {
  jaccard_similarity(hashed_shingles[[pair[1]]], hashed_shingles[[pair[2]]])
})
time <- difftime(Sys.time(), time, units = "secs")
```

\normalsize
This took up $`r object.size(hashed_shingles)`$ bytes, while storing the shingles took $`r object.size(shingles)`$ bytes. However, the whole pairwise comparison still took the same amount of time ($\approx `r round(time/(60), 2)`$ minutes).

# Similarity preserving summaries of sets

- Sets of shingles are large (larger than the original document)
\vfill
- If we have millions of documents, it may not be possible to store all the shingle-sets in memory
\vfill
- We can replace large sets by smaller representations, called *signatures*
\vfill
- And use these signatures to **approximate** Jaccard similarity
\vfill

# Characteristic matrix

In order to get a signature of our data set, we first build a *characteristic matrix* 

Columns correspond to records and the rows correspond to all hashed shingles

```{r characteristic, cache=TRUE}
# return if an item is in a list
item_in_list <- function(item, list) {
  as.integer(item %in% list) 
}

# get the characteristic matrix
# items are all the unique hash values
# columns will be each record
# we want to keep track of where each hash is included 
char_mat <- data.frame(item = unique(unlist(hashed_shingles)))

contained <- lapply(hashed_shingles, function(col) {
  vapply(char_mat$item, FUN = item_in_list, FUN.VALUE = integer(1), list = col)
})

char_mat <- do.call(cbind, contained)
rownames(char_mat) <- unique(unlist(hashed_shingles))
colnames(char_mat) <- paste("Record", seq_len(nrow(dat)))

kable(char_mat[10:15, 1:5])
```

The result is a $`r dim(char_mat)[1]`\times `r dim(char_mat)[2]`$ matrix.

**Question: **Why would we not store the data as a characteristic matrix?

# Minhashing

Want create the signature matrix through minhashing

1. Permute the rows of the characteristic matrix 
\vspace{.2in}
2. Iterate over each column of the permuted matrix 
\vspace{.2in}
3. Populate the signature matrix, row-wise, with the row index from the first `1` value found in the column 

The signature matrix is a hashing of values from the permuted characteristic matrix and has one row for the number of permutations calculated, and a column for each record

# Minhashing (cont'd)

```{r minhash-1, cache=TRUE}
# set seed for reproducibility
set.seed(02082018)

# function to get signature for 1 permutation
get_sig <- function(char_mat) {
  # get permutation order
  permute_order <- sample(seq_len(nrow(char_mat)))
  
  # get min location of "1" for each column (apply(2, ...))
  t(apply(char_mat[permute_order, ], 2, function(col) min(which(col == 1))))
}

# repeat many times
m <- 360
sig_mat <- matrix(NA, nrow=m, ncol=ncol(char_mat))
for(i in 1:m) {
  sig_mat[i, ] <- get_sig(char_mat)
}
colnames(sig_mat) <- colnames(char_mat)

# inspect results
kable(sig_mat[1:10, 1:5])
```

# Signature matrix and Jaccard similarity

The relationship between the random permutations of the characteristic matrix and the Jaccard Similarity is
$$
Pr\{\min[h(A)] = \min[h(B)]\} = \frac{|A \cap B|}{|A \cup B|}
$$

\vfill

We use this relationship to **approximate** the similarity between any two records 

\vfill
We look down each column of the signature matrix, and compare it to any other column

\vfill
The number of agreements over the total number of combinations is an approximation to Jaccard measure

\vfill

# Jaccard similarity approximation

```{r jaccard-sig, fig.height=4, cache=TRUE}
# add jaccard similarity approximated from the minhash to compare
# number of agreements over the total number of combinations
hashed_jaccard$similarity_minhash <- apply(hashed_jaccard, 1, function(row) {
  sum(sig_mat[, row[["record1"]]] == sig_mat[, row[["record2"]]])/nrow(sig_mat)
})

# how far off is this approximation?
qplot(hashed_jaccard$similarity_minhash - hashed_jaccard$similarity) +
  xlab("Difference between Jaccard similarity and minhash approximation")
```

Used minhashing to get an approximation to the Jaccard similarity, which helps by allowing us to store less data (hashing) and avoid storing sparse data (signature matrix)

We still haven't addressed the issue of **pairwise comparisons**.

# LSH (avoid pairwise comparisons)

# Banding and buckets

# Your turn 

banding in R

# Putting it all together

Choosing shingle size - somewhere?

# "Easy" LSH in R

# Evaluation

# Your turn

perform LSH and evaluate how we did


